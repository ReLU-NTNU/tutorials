{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Deep Learning and Neural Networks </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_theme(style=\"dark\")\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Analysis </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path.cwd().parent.parent / \"data\" / \"diabetes_india\" / \"diabetes.csv\"\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Age\"].hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Glucose\"].hist();   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"BMI\"].hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Outcome\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Linear vs non-linear compression  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.Tensor(df.to_numpy()[:,:-1])\n",
    "y = torch.Tensor(df.to_numpy()[:,-1])\n",
    "N, d = X.shape\n",
    "N, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_split(X, y):\n",
    "\n",
    "    N, _ = X.shape\n",
    "    idx = torch.randperm(N)\n",
    "    train_idx = idx[:int(0.8*N)]\n",
    "    val_idx = idx[int(0.8*N):int(0.9*N)]\n",
    "    test_idx = idx[int(0.9*N):]\n",
    "\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_val, y_val = X[val_idx], y[val_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = train_test_val_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X: torch.Tensor, out_dim: int) -> torch.Tensor:\n",
    "\n",
    "    X = (X - X.mean(dim=0)) / X.std(dim=0)\n",
    "    sigma = X.T @ X\n",
    "    lam, V = torch.linalg.eig(sigma)\n",
    "    lam, V = lam.real, V.real\n",
    "    idx = torch.argsort(lam, descending=True)\n",
    "    lam = lam[idx]\n",
    "    V = V[:, idx]\n",
    "\n",
    "    return X @ V[:,:out_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pca(X_train, 2)\n",
    "X_val_pca = pca(X_val, 2)\n",
    "X_test_pca = pca(X_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_and_normalize(X):\n",
    "    return (X - X.mean(dim=0)) / X.std(dim=0)\n",
    "\n",
    "def train_classifier(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        X_val,\n",
    "        y_val, \n",
    "        model, \n",
    "        epochs=10000, \n",
    "        lr=0.01,\n",
    "        early_stop=10):\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_losses, val_losses = [], []\n",
    "    min_val_loss, early_stop_counter, best_model = float(\"inf\"), 0, None\n",
    "\n",
    "    X_train = center_and_normalize(X_train)\n",
    "    X_val = center_and_normalize(X_val)\n",
    "\n",
    "    for _ in tqdm(range(epochs)):\n",
    "\n",
    "        if early_stop_counter == early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X_train).squeeze()\n",
    "        loss = criterion(logits, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "\n",
    "            y_pred_val = model(X_val).squeeze()\n",
    "            val_loss = criterion(y_pred_val, y_val)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "            min_val_loss = min(min_val_loss, val_loss.item())\n",
    "            if min_val_loss == val_loss.item():\n",
    "                best_model = model\n",
    "                early_stop_counter = 0\n",
    "\n",
    "                probs = torch.sigmoid(y_pred_val)\n",
    "                preds = (probs > 0.5).float()\n",
    "\n",
    "                metrics = {\n",
    "                    \"accuracy\": accuracy_score(y_val, preds),\n",
    "                    \"precision\": precision_recall_fscore_support(y_val, preds, average=\"binary\")[0],\n",
    "                    \"recall\": precision_recall_fscore_support(y_val, preds, average=\"binary\")[1],\n",
    "                    \"f1\": precision_recall_fscore_support(y_val, preds, average=\"binary\")[2]\n",
    "                }\n",
    "\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "\n",
    "    return best_model, metrics, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressor(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(in_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.linear(x)\n",
    "    \n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegressor(2)\n",
    "logistic, metrics, train_losses, valid_losses = train_classifier(X_train_pca, y_train, X_val_pca, y_val, logistic)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(2)\n",
    "mlp, metrics, train_losses, valid_losses = train_classifier(X_train_pca, y_train, X_val_pca, y_val, mlp)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_probs = logistic(center_and_normalize(X_test_pca)).squeeze()\n",
    "mlp_probs = logistic(center_and_normalize(X_test_pca)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(ncols=3)\n",
    "\n",
    "ax[0].scatter(\n",
    "    X_test_pca[:, 0].detach().numpy(), \n",
    "    X_test_pca[:, 1].detach().numpy(),\n",
    "    c=logistic_probs.detach().cpu().numpy(),\n",
    "    cmap='coolwarm');\n",
    "ax[0].set_title(\"Logistic predictions\")\n",
    "\n",
    "ax[1].scatter(\n",
    "    X_test_pca[:, 0].detach().numpy(),\n",
    "    X_test_pca[:, 1].detach().numpy(),\n",
    "    c=mlp_probs.detach().cpu().numpy(),\n",
    "    cmap='coolwarm');\n",
    "ax[1].set_title(\"Neural predictions\")\n",
    "\n",
    "ax[2].scatter(\n",
    "    X_test_pca[:, 0].detach().numpy(), \n",
    "    X_test_pca[:, 1].detach().numpy(), \n",
    "    c=y_test.detach().cpu().numpy(),\n",
    "    cmap='coolwarm');\n",
    "\n",
    "ax[2].set_title(\"True labels\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
