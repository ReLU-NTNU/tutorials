{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Linear models, matrix calculus, and gradient descent </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us understand machine learning through one of the simplest models possible, the linear regression model. Despite its simplicity, it is one of the most used models in machine learning (even modern ML). It is often used as a component in neural networks or on its own. \n",
    "\n",
    "The optimisation problem associated with this model is known as least squares. We are essentially trying to map a feature vector to some value by multiplying the features by learnable weights, and then summing the result. For instance, we might be interested in predicting home prices, and the features may be size of home, average price in area, and distance from the nearest city centre. One might find the weights to be [0,1,0] if the average price in the area is exactly the price of the home in our dataset. \n",
    "\n",
    "Mathematically, we are given a data matrix $A \\in \\mathbb{R}^{m \\times n}$ and a vector of outcomes $y \\in \\mathbb{R}^m$, and attempt to\n",
    "find a parameter vector $x \\in \\mathbb{R}^n$ which minimizes the residual $ \\left\\lVert A x - y \\right\\rVert_2^2 $. This finds what weight vector $x$ we can multiply with the data matrix $A$ to make the result as close to the prediction target as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Solving the least-squares problem with matrix calculus </h2>\n",
    "\n",
    "There are a lot of ways to solve this problem. In this notebook, we will consider two ways. The first is calculating the closed-form solution through matrix calculus. You can decompose it into sums, but it is quicker and cleaner to stay at the matrix level. \n",
    "\n",
    "To solve it, we first need to understand the concepts of a gradient and a jacobian. The gradient is equivalent to the first derivative in the multivariate world, and the jacobian is equivalent to the second derivative. Here are the definitions:\n",
    "\n",
    "**Definition 1: Gradient**\n",
    "\n",
    "Let $ f : \\mathbb{R}^n \\to \\mathbb{R} $ be a differentiable function. The *gradient* of $ f $ is the function $ \\nabla f : \\mathbb{R}^n \\to \\mathbb{R}^n $ defined by\n",
    "$$\n",
    "\\nabla f(\\vec{x}) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1} (\\vec{x}) \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f}{\\partial x_n} (\\vec{x})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Definition 2: Jacobian**\n",
    "\n",
    "Let $\\vec{f} : \\mathbb{R}^n \\to \\mathbb{R}^m$ be a differentiable function. The Jacobian of $ \\vec{f} $ is the function $ D\\vec{f} : \\mathbb{R}^n \\to \\mathbb{R}^{m \\times n} $ defined as:\n",
    "\n",
    "$$\n",
    "D\\vec{f}(\\vec{x}) = \n",
    "\\begin{bmatrix}\n",
    "\\nabla f_1(\\vec{x})^\\top \\\\\n",
    "\\vdots \\\\\n",
    "\\nabla f_m(\\vec{x})^\\top\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1}(\\vec{x}) & \\cdots & \\frac{\\partial f_1}{\\partial x_n}(\\vec{x}) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1}(\\vec{x}) & \\cdots & \\frac{\\partial f_m}{\\partial x_n}(\\vec{x})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The jacobian of the gradient of a function is known as the Hessian of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Using the gradient and jacobian to solve least-squares </h3>\n",
    "\n",
    "As you can see, the gradient is a vector analogue to the classic univariate first derivative. The vector is defined where a single value (scalar) output depends on a vector of parameters. In machine learning, this is relevant when we have computed a loss value from predictions created by the parameters of a model. With least-squares, this is exactly what we are dealing with. The loss is calculated as $ \\left\\lVert A x - y \\right\\rVert_2^2 $ (note that the L2 norm gives a scalar output), and we used the parameters $ x $ to calculate the predictions. \n",
    "\n",
    "How do we use this to find the optimal parameters? We need to find the parameters that minimise the loss function. Further, the technique is completely analoguous to what is done in the univariate case (from math R1 and R2). For the univariate case, we set the derivative with respect to the parameters to 0, and calculate the parameter value that solves the equation. For the multivariate case, we set the gradient to the 0 vector, that is the vector with each entry equal to 0. Instead of a single equation, we get one equation for each of the $n$ parameters. These we can solve. \n",
    "\n",
    "However, how do we know that the parameters that lead to a zero gradient are the ones that minimise the loss? Again, the answer is the same as in the univariate case. We need to prove that the loss function is convex with respect to the parameters. Remember that a univariate convex function is defined by the second derivative being non-negative (bending upwards). How does this translate to the multivariate case? The answer is that the hessian needs to be positive-semidefinite (which is basically the non-negative in matrix world). \n",
    "\n",
    "A matrix $\\mathbf{A}$ is positive semidefinite if it is symmetric and for any vector $\\mathbf{x}, \\mathbf{x}^T \\mathbf{A} \\mathbf{x} \\geq 0$. This is equivalent to saying all eigenvalues must be non-negative (don't worry if you haven't learnt about this yet). You can get some intuition about PSD-matrices by considering diagonal matrices. That is, a matrix where all non-diagonal elements are equal to 0. In this case, the eigenvalues are the elements on the diagonal, and an all non-negative diagonal implies the matrix is PSD. We can interpet this is a \"non-negative matrix\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Finding the optimal parameters with matrix calculus </h3>\n",
    "\n",
    "We can solve the least-squares problem in two steps:\n",
    "\n",
    "1. Prove that the hessian of the loss function is positive-semidefinite (\"the second derivative is non-negative\")\n",
    "2. Find the parameters that yield a gradient equal to 0\n",
    "\n",
    "If you want, you can solve it yourself and then come back to the notebook. Otherwise, the solution is below.\n",
    "\n",
    "We skip step 1 for now, but there are good solutions available on the web. We go straight to finding the parameters that minimise the loss, by setting the gradient equal to the 0 vector. \n",
    "\n",
    "The objective function for OLS is given by:\n",
    "\n",
    "$$\n",
    "f(x) = \\left\\lVert A x - y \\right\\rVert_2^2 = (A x - y)^T (A x - y)\n",
    "$$\n",
    "\n",
    "Expanding the quadratic form (using matrix algebra that you might not have seen in your courses yet), we have:\n",
    "\n",
    "$$\n",
    "f(x) = x^T A^T A x - 2 y^T A x + y^T y\n",
    "$$\n",
    "\n",
    "The gradient of $f(x)$ with respect to $x$ is:\n",
    "\n",
    "$$\n",
    "\\nabla_x f(x) = 2 A^T A x - 2 A^T y\n",
    "$$\n",
    "\n",
    "To find the optimal $x$, set the gradient to zero and solve for $x$:\n",
    "\n",
    "$$\n",
    "2 A^T A x - 2 A^T y = 0\n",
    "$$\n",
    "\n",
    "Simplifying, we get:\n",
    "\n",
    "$$\n",
    "A^T A x = A^T y\n",
    "$$\n",
    "\n",
    "Assuming $A^T A$ is invertible, solve for $x$:\n",
    "\n",
    "$$\n",
    "x = (A^T A)^{-1} A^T y\n",
    "$$\n",
    "\n",
    "There we have it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Solving the least-squares problem with gradient descent </h2>\n",
    "\n",
    "An alternative solution method is through gradient descent. We introduced GD in the slides, but for a more comprehensive review, see https://ds100.org/fa23/lecture/lec13/, or https://people.eecs.berkeley.edu/~jrs/189/lec/03.pdf. Summarized:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\nabla J(\\mathbf{w}_t)\n",
    "$$\n",
    "\n",
    "Now its time to apply these techniques to a real world dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë®‚Äçüíª Task 1: What is the relationship between hours studied and exam results?\n",
    "\n",
    "Some might say that the more hous you log, the better your exam scores are. Let's find out how true that is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('regression_data/mission1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement Linear Regression from scratch\n",
    "class LinearRegression:\n",
    "    def __init__(self, fit_method=\"closed_form\"):\n",
    "        \n",
    "        if fit_method not in [\"closed_form\", \"gradient_descent\"]:\n",
    "            raise ValueError(\"fit_method must be either 'closed_form' or 'gradient_descent'\")\n",
    "\n",
    "        self.fit_method = fit_method\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        if self.fit_method == \"closed_form\":\n",
    "            # Implement closed form solution\n",
    "            ...\n",
    "        elif self.fit_method == \"gradient_descent\":\n",
    "            # Implement gradient descent solution\n",
    "            ...\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the data\n",
    "# TODO: Implement Linear Regression\n",
    "# TODO: Estimate the coefficients (effect of studying) through Linear Regression\n",
    "# TODO: Visualize the regression line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üß† Task 2: Logistic Regression for Binary Classification\n",
    "\n",
    "### Mission\n",
    "1. From your Linear Regression model, make a Logistic Regression model that performs binary classification\n",
    "2. Look into how you can use feature engineering to improve the accuracy\n",
    "3. Solve it using a Decision Tree instead, and compare the results\n",
    "\n",
    "**Performance goal**: 0.88 accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('regression_data/mission2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement Logistic Regression from scratch\n",
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the data. Is it linearly separable?\n",
    "# TODO: What can be done to make it linearly separable?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
