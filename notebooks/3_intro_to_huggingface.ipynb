{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Introduction to Hugging Face </h1>\n",
    "\n",
    "Hugging Face is the worlds largest provider of open source machine learning software. They have implementations of a wide variety of models (and utilities such as a data processing library). They essentially provide implementations of PyTorch models, like the one you built in the previous notebook. That means you can use them exactly like you would normal PyTorch models, and train them exactly like in the last notebook.\n",
    "\n",
    "In this notebook, we will look at text summarization with Hugging Face. Specifically, we will use their transformers library, which is their main library for language processing. However, we encourage you to look into the vast amount of other tasks that can be solved (pretty easily) with Hugging Face, such as advanced tasks like image generation (see the diffusers library) and text-to-speech, or simpler tasks like classification and next-word prediction. \n",
    "\n",
    "Anyone can train a model and upload it to Hugging Face. Large companies such as Meta and Microsoft have uploaded models to Hugging Face. Here is a link to available models (https://huggingface.co/models). After this tutorial you will be able to use these models, as well as tune them on a specific task that you want to solve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Using a pretrained model to summarize text </h2>\n",
    "\n",
    "We are now going to use a large pretrained transformer to generate text summaries. We are going to use a model from Facebook called bart-large-cnn. Here is a link to the model documentation: https://huggingface.co/facebook/bart-large-cnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"facebook/bart-large-cnn\"\n",
    "\n",
    "# Create a summarizer\n",
    "summarizer = ...\n",
    "\n",
    "# Find some long English text online and put it here\n",
    "text = ...\n",
    "\n",
    "# Generate the summary\n",
    "summary = ...\n",
    "\n",
    "# Print the summary\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It really is that simple to solve many machine learning tasks when we have a large open source library like transformers. Companies such as Meta and Google have spent much resources to build and train these models, and we can conveniently use them. However, the models are usually trained on a general objective, and when we are dealing with specific tasks, we should finetune them. \n",
    "\n",
    "Let's see how well the model performs on Norwegian text. Find some Norwegian text online and paste it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste some Norwegian text\n",
    "text = ...\n",
    "\n",
    "# Generate the summary\n",
    "summary = ...\n",
    "\n",
    "# Print the summary\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Finetuning </h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the model used on Norwegian text wasn't that bad. Still, it can be improved, so let's achieve this by finetuning it. We will tune the model on a large dataset of Norwegian text.\n",
    "\n",
    "Let's start by fetching the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally you would just use the load_dataset function from Hugging Face to fetch the data,\n",
    "# but it didn't work and we needed to download the files\n",
    "summary_data_path = Path.cwd().parent / \"data\" / \"norwegian_summarization\"\n",
    "train_data = Dataset.from_file(str(summary_data_path / \"train.arrow\"))\n",
    "validation_data = Dataset.from_file(str(summary_data_path / \"validation.arrow\"))\n",
    "test_data = Dataset.from_file(str(summary_data_path / \"test.arrow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0][\"document\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0][\"summary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Now we leave the tuning part to you. Some things you will need: </h3>\n",
    "\n",
    "1. A tokenizer that converts the text into tokens (Tokens correspond to words, but sometimes the words are split into multiple tokens. The tokens are represented by integers. We recommend you print the output of the tokenizer to inspect how it works)\n",
    "2. Some data preprocessing function\n",
    "3. A function to compute metrics and evaluate the summarizer (you can look into the Rouge score)\n",
    "4. A training function (you can use a Hugging Face API, or train it in the PyTorch way with a function very similar to what you built in the last notebook)\n",
    "\n",
    "This exercise is meant to make you familiar with working independently with Hugging Face libraries. That usually means Googling and looking through Hugging Face documentation. Here is a good resource: https://huggingface.co/learn/nlp-course/chapter7/5?fw=pt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Finished? Here are some more tasks for you (all can be solved with Hugging Face): </h3>\n",
    "\n",
    "- Generate an image of a skateboarding turtle (keywords: text to image, stable diffusion).\n",
    "- Find a news article online, cut away the last half, and then regenerate the last half with a transformer. Compare how similar the real and generated parts are (keywords: next token prediction). \n",
    "- Download some image online, remove 30% of it, and regenerate the 30% with a diffusion model (keywords: diffusion inpainting).\n",
    "\n",
    "These tasks sound a little crazy, but Hugging Face does the heavy lifting (you don't need to train any models, the tasks can be solved with pretrained ones)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
