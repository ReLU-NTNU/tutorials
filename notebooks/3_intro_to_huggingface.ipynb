{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Introduction to Hugging Face </h1>\n",
    "\n",
    "Hugging Face is the worlds largest provider of open source machine learning software. They have implementations of a wide variety of models (and utilities such as a data processing library). They provide implementations of PyTorch models, like the one you built in the notebook from the kick-off weekend. That means you can use them exactly like you would normal PyTorch models, and train them exactly like in the kick-off notebook.\n",
    "\n",
    "We will now look at solving a variety of tasks with Hugging Face, including text summarization and image generation. Specifically, we will use the libraries Transformers and Diffusers.\n",
    "\n",
    "Anyone can train a model and upload it to Hugging Face. Companies such as Meta and Microsoft have uploaded models to Hugging Face. Here is a link to available models (https://huggingface.co/models). After this tutorial you will be able to use these models, as well as tune them on a specific task that you want to solve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Using a pretrained model to summarize text </h2>\n",
    "\n",
    "We are now going to use a large pretrained transformer to generate text summaries. We are going to use a model from Facebook called bart-large-cnn. Here is a link to the model documentation: https://huggingface.co/facebook/bart-large-cnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"facebook/bart-large-cnn\"\n",
    "\n",
    "# Create a summarizer\n",
    "summarizer = ...\n",
    "\n",
    "# Find some long English text online and put it here\n",
    "text = ...\n",
    "\n",
    "# Generate the summary\n",
    "summary = ...\n",
    "\n",
    "# Print the summary\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It really is that simple to solve many machine learning tasks when we have a large open source library like transformers. Companies such as Meta and Google have spent much resources to build and train these models, and we can conveniently use them. However, the models are usually trained on a general objective, and when we are dealing with specific tasks, we should finetune them. \n",
    "\n",
    "Let's see how well the model performs on Norwegian text. Find some Norwegian text online and paste it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste some Norwegian text\n",
    "text = ...\n",
    "\n",
    "# Generate the summary\n",
    "summary = ...\n",
    "\n",
    "# Print the summary\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> More tasks with Diffusers and Transformers </h2>\n",
    "\n",
    "Now we will solve some new tasks with the HF libraries, to showcase their versatility and power. We already tried the transformers library. Now we will look at the Diffusers library as well. Diffusers is all about generating images, audio, and similar data (through diffusion models). More info about Diffusers: https://huggingface.co/docs/diffusers/index. \n",
    "\n",
    "<h3> Here are some tasks (all can be solved with Hugging Face): </h3>\n",
    "\n",
    "- Generate an image of a skateboarding turtle (keywords: text to image, stable diffusion).\n",
    "- Find a news article online, cut away the last half, and then regenerate the last half with a transformer. Compare how similar the real and generated parts are (keywords: next word prediction). \n",
    "- Download some image online, remove 30% of it, and regenerate the 30% (keywords: diffusion inpainting).\n",
    "\n",
    "These tasks sound a little crazy, but Hugging Face does the heavy lifting (you don't need to train any models, the tasks can be solved with pretrained ones. Also, Google is your friend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code the turtle generator here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code the news completion here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code the image inpainting here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Finetuning </h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that most of the models uploaded to Hugging Face have been trained on a general task that may not specifically correspond to what you want to do. In order to leverage what these models have learnt during large scale pretraining, we can tune them. This means that we take the pretrained model (its architecture and weights), and continue the training on our selected task. \n",
    "\n",
    "Let's return to the summarization model. How did it perform on Norwegian text? Maybe it wasn't that bad. Still, it can be improved, so let's achieve this by finetuning it. We will tune the model on a large dataset of Norwegian text.\n",
    "\n",
    "Let's start by fetching the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = \"https://huggingface.co/datasets/NbAiLab/norwegian-xsum/resolve/main/nob/train/data-00000-of-00001.arrow?download=true\"\n",
    "valid_url = \"https://huggingface.co/datasets/NbAiLab/norwegian-xsum/resolve/main/nob/validation/data-00000-of-00001.arrow?download=true\"\n",
    "test_url = \"https://huggingface.co/datasets/NbAiLab/norwegian-xsum/resolve/main/nob/test/data-00000-of-00001.arrow?download=true\"\n",
    "train_path, valid_path, test_path = \"train.arrow\", \"valid.arrow\", \"test.arrow\"\n",
    "\n",
    "for p, url in [(train_path, train_url), (valid_path, valid_url), (test_path, test_url)]:\n",
    "    urlretrieve(url, p)\n",
    "\n",
    "train_data = Dataset.from_file(train_path)\n",
    "validation_data = Dataset.from_file(valid_path)\n",
    "test_data = Dataset.from_file(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0][\"document\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0][\"summary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Now we leave the tuning part to you. Some things you will need: </h3>\n",
    "\n",
    "1. A tokenizer that converts the text into tokens (Tokens correspond to words, but sometimes the words are split into multiple tokens. The tokens are represented by integers. We recommend you print the output of the tokenizer to inspect how it works)\n",
    "2. Some data preprocessing function\n",
    "3. A function to compute metrics and evaluate the summarizer (you can look into the Rouge score)\n",
    "4. A training function (you can use a Hugging Face API, or train it in the PyTorch way with a function very similar to what you built in the last notebook)\n",
    "\n",
    "This exercise is meant to make you familiar with working independently with Hugging Face libraries. That usually means Googling and looking through Hugging Face documentation. Here is a good resource: https://huggingface.co/learn/nlp-course/chapter7/5?fw=pt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
