{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Introduction to Hugging Face </h1>\n",
    "\n",
    "Hugging Face is the worlds largest provider of open source machine learning software. They have implementations of a wide variety of models (and utilities such as a data processing library). They provide implementations of PyTorch models, like the one you built in the notebook from the kick-off weekend. That means you can use them exactly like you would normal PyTorch models, and train them exactly like in the kick-off notebook.\n",
    "\n",
    "We will now look at solving a variety of tasks with Hugging Face, including text summarization and image generation. Specifically, we will use the libraries Transformers and Diffusers.\n",
    "\n",
    "Anyone can train a model and upload it to Hugging Face. Companies such as Meta and Microsoft have uploaded models to Hugging Face. Here is a link to available models (https://huggingface.co/models). After this tutorial you will be able to use these models, as well as tune them on a specific task that you want to solve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Using a pretrained model to summarize text </h2>\n",
    "\n",
    "We are now going to use a large pretrained transformer to generate text summaries. We are going to use a model from Facebook called bart-large-cnn. Here is a link to the model documentation: https://huggingface.co/facebook/bart-large-cnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = \"facebook/bart-large-cnn\"\n",
    "\n",
    "# Create a summarizer\n",
    "summarizer = ...\n",
    "\n",
    "# Find some long English text online and put it here\n",
    "text = ...\n",
    "\n",
    "# Generate the summary\n",
    "summary = ...\n",
    "\n",
    "# Print the summary\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It really is that simple to solve many machine learning tasks when we have a large open source library like transformers. Companies such as Meta and Google have spent much resources to build and train these models, and we can conveniently use them. However, the models are usually trained on a general objective, and when we are dealing with specific tasks, we should finetune them. \n",
    "\n",
    "Let's see how well the model performs on Norwegian text. Find some Norwegian text online and paste it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paste some Norwegian text\n",
    "text = ...\n",
    "\n",
    "# Generate the summary\n",
    "summary = ...\n",
    "\n",
    "# Print the summary\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> More tasks with Diffusers and Transformers </h2>\n",
    "\n",
    "Now we will solve some new tasks with the HF libraries, to showcase their versatility and power. We already tried the transformers library. Now we will look at the Diffusers library as well. Diffusers is all about generating images, audio, and similar data (through diffusion models). More info about Diffusers: https://huggingface.co/docs/diffusers/index. \n",
    "\n",
    "<h3> Here are some tasks (all can be solved with Hugging Face): </h3>\n",
    "\n",
    "- Generate an image of a skateboarding turtle (keywords: text to image, stable diffusion).\n",
    "- Find a news article online, cut away the last half, and then regenerate the last half with a transformer. Compare how similar the real and generated parts are (keywords: next word prediction). \n",
    "- Download some image online, remove 30% of it, and regenerate the 30% (keywords: diffusion inpainting).\n",
    "\n",
    "These tasks sound a little crazy, but Hugging Face does the heavy lifting (you don't need to train any models, the tasks can be solved with pretrained ones. Also, Google is your friend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code the turtle generator here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code the news completion here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code the image inpainting here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Finetuning </h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that most of the models uploaded to Hugging Face have been trained on a general task that may not specifically correspond to what you want to do. In order to leverage what these models have learnt during large scale pretraining, we can tune them. This means that we take the pretrained model (its architecture and weights), and continue the training on our selected task. \n",
    "\n",
    "Let's return to the summarization model. How did it perform on Norwegian text? Maybe it wasn't that bad. Still, it can be improved, so let's achieve this by finetuning it. We will tune the model on a large dataset of Norwegian text.\n",
    "\n",
    "Let's start by fetching the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = \"https://huggingface.co/datasets/NbAiLab/norwegian-xsum/resolve/main/nob/train/data-00000-of-00001.arrow?download=true\"\n",
    "valid_url = \"https://huggingface.co/datasets/NbAiLab/norwegian-xsum/resolve/main/nob/validation/data-00000-of-00001.arrow?download=true\"\n",
    "test_url = \"https://huggingface.co/datasets/NbAiLab/norwegian-xsum/resolve/main/nob/test/data-00000-of-00001.arrow?download=true\"\n",
    "train_path, valid_path, test_path = \"train.arrow\", \"valid.arrow\", \"test.arrow\"\n",
    "\n",
    "for p, url in [(train_path, train_url), (valid_path, valid_url), (test_path, test_url)]:\n",
    "    urlretrieve(url, p)\n",
    "\n",
    "train_data = Dataset.from_file(train_path)\n",
    "validation_data = Dataset.from_file(valid_path)\n",
    "test_data = Dataset.from_file(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['document', 'summary', 'id'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'35232142'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Det er fortsatt pågående reparasjonsarbeid i Hawick og mange veier i Peeblesshire er fortsatt sterkt påvirket av stillestående vann. Tog på vestkysten har blitt forstyrret på grunn av skader ved Lamington Viaduct. Mange bedrifter og husholdere ble rammet av flom i Newton Stewart etter at elven Cree overvannet i byen. Jeanette Tate, som eier Cinnamon Cafe, som ble sterkt rammet, sa at hun ikke kunne klandre multi-agenturresponsen etter at flommen rammet. Hun sa imidlertid at mer forebyggende arbeid kunne blitt utført for å sikre at støttemuren ikke sviktet. \"Det er vanskelig, men jeg tror det er så mye publisitet for Dumfries og Nith - og jeg setter helt pris på det - men det er nesten som om vi er forsømmet eller glemt\", sa hun. \"Det er kanskje ikke sant, men det er kanskje mitt perspektiv de siste dagene. \"Hvorfor var du ikke klar til å hjelpe oss litt mer når advarselen og alarmen var ute?\" I mellomtiden er det fortsatt en overflodsalarm på tvers av grensen på grunn av den konstante regnen. Peebles ble hardt rammet av problemer, noe som utløste krav om å innføre mer forsvar i området. Scottish Borders Council har lagt en liste på sin nettside over de veiene som er mest berørt, og sjåførene har blitt oppfordret til ikke å ignorere stengningsskilt. Labour-partiets vise skotsk leder Alex Rowley var i Hawick mandag for å se situasjonen fra første hånd. Han sa at det var viktig å få overflodsskyddsplanen riktig, men støttet krav om å fremskynde prosessen. \"Jeg ble ganske overrasket over hvor mye skade som har blitt gjort\", sa han. \"Det er åpenbart hjerteskjærende for folk som har blitt tvunget ut av sine hjem og virkningen på virksomheten\". Han sa det var viktig at \"med umiddelbare skritt\" ble tatt for å beskytte de mest sårbare områdene og en klar tidsplan ble satt i gang for å forhindre oversvømmelser. Har du blitt påvirket av oversvømmelser i Dumfries og Galloway eller grensene? Fortell oss om din erfaring med situasjonen og hvordan den ble håndtert. Send oss en e-post på selkirk.news@bbc.co.uk eller dumfries@bbc.co.uk.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][\"document\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rengjøringsoperasjoner fortsetter over de skotske grensene og Dumfries og Galloway etter oversvømmelser forårsaket av Storm Frank.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][\"summary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Now we leave the tuning part to you. Some things you will need: </h3>\n",
    "\n",
    "1. A tokenizer that converts the text into tokens (Tokens correspond to words, but sometimes the words are split into multiple tokens. The tokens are represented by integers. We recommend you print the output of the tokenizer to inspect how it works)\n",
    "2. Some data preprocessing function\n",
    "3. A function to compute metrics and evaluate the summarizer (you can look into the Rouge score)\n",
    "4. A training function (you can use a Hugging Face API, or train it in the PyTorch way with a function very similar to what you built in the last notebook)\n",
    "\n",
    "This exercise is meant to make you familiar with working independently with Hugging Face libraries. That usually means Googling and looking through Hugging Face documentation. Here is a good resource: https://huggingface.co/learn/nlp-course/chapter7/5?fw=pt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
