{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Word Embeddings </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Tokenization </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'am', 'brown', 'doctor', 'dog', 'fox', 'i', 'jumps', 'lazy', 'over', 'programmer', 'quick', 'student', 'teacher', 'the']\n",
      "[[6, 1, 0, 12, 15, 15, 15, 15, 15], [6, 1, 0, 13, 15, 15, 15, 15, 15], [6, 1, 0, 3, 15, 15, 15, 15, 15], [6, 1, 0, 10, 15, 15, 15, 15, 15], [14, 11, 2, 5, 7, 9, 14, 8, 4]]\n",
      "9 [9, 9, 9, 9, 9]\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.vocab = None\n",
    "        self.max_len = None\n",
    "        self.vocab_size = None\n",
    "\n",
    "    def preprocess(self, text):\n",
    "\n",
    "        # remove all non alphabetic characters\n",
    "        text = ''.join(e for e in text if e.isalnum() or e.isspace())\n",
    "\n",
    "        # convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        return text\n",
    "\n",
    "    def generate_vocab(self, texts):\n",
    "\n",
    "        text_preprocessed = [self.preprocess(text) for text in texts]\n",
    "\n",
    "        self.max_len = max([len(seq.split()) for seq in text_preprocessed])\n",
    "\n",
    "        words = \" \".join(text_preprocessed).split()\n",
    "\n",
    "        vocab = list(set(words))\n",
    "\n",
    "        vocab.sort()\n",
    "\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.vocab_size = len(self.vocab) + 1\n",
    "    \n",
    "    def tokenize(self, texts):\n",
    "\n",
    "        total_tokens = []\n",
    "\n",
    "        for text in texts:\n",
    "    \n",
    "            text_preprocessed = self.preprocess(text)\n",
    "\n",
    "            words = text_preprocessed.split()\n",
    "\n",
    "            tokens = []\n",
    "\n",
    "            for word in words:\n",
    "                tokens.append(self.vocab.index(word))\n",
    "            \n",
    "            tokens += [self.vocab_size-1 for _ in range(self.max_len - len(tokens))]\n",
    "\n",
    "            total_tokens.append(tokens)\n",
    "\n",
    "        return total_tokens\n",
    "\n",
    "texts = [\n",
    "    \"I am a student\", \n",
    "    \"I am a teacher\", \n",
    "    \"I am a doctor\", \n",
    "    \"I am a programmer\", \n",
    "    \"The quick brown fox jumps over the lazy dog\"\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.generate_vocab(texts)\n",
    "\n",
    "print(tokenizer.vocab)\n",
    "\n",
    "tokens = tokenizer.tokenize(texts)\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "print(tokenizer.max_len, [len(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Embedding Layer </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "[[6, 1, 0, 12, 15, 15, 15, 15, 15], [6, 1, 0, 13, 15, 15, 15, 15, 15], [6, 1, 0, 3, 15, 15, 15, 15, 15], [6, 1, 0, 10, 15, 15, 15, 15, 15], [14, 11, 2, 5, 7, 9, 14, 8, 4]]\n",
      "tensor([[[ 0.3276,  0.2250,  0.4126,  0.3643,  0.1932,  1.5503,  0.0839,\n",
      "           0.4204,  0.6827, -0.4537],\n",
      "         [ 0.0082, -0.6683,  1.0538,  1.8405,  0.2528, -1.9614,  0.0260,\n",
      "          -1.1189, -0.6230, -0.2543],\n",
      "         [-0.5169,  0.2792, -1.3118,  0.3313, -0.7539, -0.0291,  1.9711,\n",
      "          -0.2230,  0.0837, -1.6425],\n",
      "         [ 0.6643, -1.3734, -0.9785, -1.9924, -0.5671,  1.0187,  0.4730,\n",
      "           0.8144, -1.4467,  0.5170],\n",
      "         [ 1.0626, -0.1594, -0.7191,  0.0479,  2.0683,  2.2863,  0.9569,\n",
      "          -0.9163, -0.3195, -1.4508],\n",
      "         [ 1.0626, -0.1594, -0.7191,  0.0479,  2.0683,  2.2863,  0.9569,\n",
      "          -0.9163, -0.3195, -1.4508],\n",
      "         [ 1.0626, -0.1594, -0.7191,  0.0479,  2.0683,  2.2863,  0.9569,\n",
      "          -0.9163, -0.3195, -1.4508],\n",
      "         [ 1.0626, -0.1594, -0.7191,  0.0479,  2.0683,  2.2863,  0.9569,\n",
      "          -0.9163, -0.3195, -1.4508],\n",
      "         [ 1.0626, -0.1594, -0.7191,  0.0479,  2.0683,  2.2863,  0.9569,\n",
      "          -0.9163, -0.3195, -1.4508]],\n",
      "\n",
      "        [[ 0.3276,  0.2250,  0.4126,  0.3643,  0.1932,  1.5503,  0.0839,\n",
      "           0.4204,  0.6827, -0.4537],\n",
      "         [ 0.0082, -0.6683,  1.0538,  1.8405,  0.2528, -1.9614,  0.0260,\n",
      "          -1.1189, -0.6230, -0.2543],\n",
      "         [-0.5169,  0.2792, -1.3118,  0.3313, -0.7539, -0.0291,  1.9711,\n",
      "          -0.2230,  0.0837, -1.6425],\n",
      "         [-1.1241, -0.8365,  1.2287,  1.7567,  1.6321, -1.3363, -0.3031,\n",
      "           0.1299,  0.0427,  1.1551],\n",
      "         [ 1.0626, -0.1594, -0.7191,  0.0479,  2.0683,  2.2863,  0.9569,\n",
      "          -0.9163, -0.3195, -1.4508],\n",
      "         [ 1.0626, -0.1594, -0.7191,  0.0479,  2.0683,  2.2863,  0.9569,\n",
      "          -0.9163, -0.3195, -1.4508],\n",
      "         [ 1.0626, -0.1594, -0.7191,  0.0479,  2.0683,  2.2863,  0.9569,\n",
      "          -0.9163, -0.3195, -1.4508],\n",
      "         [ 1.0626, -0.1594, -0.7191,  0.0479,  2.0683,  2.2863,  0.9569,\n",
      "          -0.9163, -0.3195, -1.4508],\n",
      "         [ 1.0626, -0.1594, -0.7191,  0.0479,  2.0683,  2.2863,  0.9569,\n",
      "          -0.9163, -0.3195, -1.4508]],\n",
      "\n",
      "        [[ 0.3276,  0.2250,  0.4126,  0.3643,  0.1932,  1.5503,  0.0839,\n",
      "           0.4204,  0.6827, -0.4537],\n",
      "         [ 0.0082, -0.6683,  1.0538,  1.8405,  0.2528, -1.9614,  0.0260,\n",
      "          -1.1189, -0.6230, -0.2543],\n",
      "         [-0.5169,  0.2792, -1.3118,  0.3313, -0.7539, -0.0291,  1.9711,\n",
      "          -0.2230,  0.0837, -1.6425],\n",
      "         [-1.1400,  0.2050, -0.5642, -3.1620,  0.6350,  1.1386,  2.0465,\n",
      "          -0.1759, -0.6019, -0.6119],\n",
      "         [ 1.0626, -0.1594, -0.7191,  0.0479,  2.0683,  2.2863,  0.9569,\n",
      "          -0.9163, -0.3195, -1.4508],\n",
      "         [ 1.0626, -0.1594, -0.7191,  0.0479,  2.0683,  2.2863,  0.9569,\n",
      "          -0.9163, -0.3195, -1.4508],\n",
      "         [ 1.0626, -0.1594, -0.7191,  0.0479,  2.0683,  2.2863,  0.9569,\n",
      "          -0.9163, -0.3195, -1.4508],\n",
      "         [ 1.0626, -0.1594, -0.7191,  0.0479,  2.0683,  2.2863,  0.9569,\n",
      "          -0.9163, -0.3195, -1.4508],\n",
      "         [ 1.0626, -0.1594, -0.7191,  0.0479,  2.0683,  2.2863,  0.9569,\n",
      "          -0.9163, -0.3195, -1.4508]],\n",
      "\n",
      "        [[ 0.3276,  0.2250,  0.4126,  0.3643,  0.1932,  1.5503,  0.0839,\n",
      "           0.4204,  0.6827, -0.4537],\n",
      "         [ 0.0082, -0.6683,  1.0538,  1.8405,  0.2528, -1.9614,  0.0260,\n",
      "          -1.1189, -0.6230, -0.2543],\n",
      "         [-0.5169,  0.2792, -1.3118,  0.3313, -0.7539, -0.0291,  1.9711,\n",
      "          -0.2230,  0.0837, -1.6425],\n",
      "         [ 0.6109, -0.8235, -0.3854, -0.5245, -0.3777,  1.0281, -1.5225,\n",
      "          -1.5588, -0.0624, -1.8021],\n",
      "         [ 1.0626, -0.1594, -0.7191,  0.0479,  2.0683,  2.2863,  0.9569,\n",
      "          -0.9163, -0.3195, -1.4508],\n",
      "         [ 1.0626, -0.1594, -0.7191,  0.0479,  2.0683,  2.2863,  0.9569,\n",
      "          -0.9163, -0.3195, -1.4508],\n",
      "         [ 1.0626, -0.1594, -0.7191,  0.0479,  2.0683,  2.2863,  0.9569,\n",
      "          -0.9163, -0.3195, -1.4508],\n",
      "         [ 1.0626, -0.1594, -0.7191,  0.0479,  2.0683,  2.2863,  0.9569,\n",
      "          -0.9163, -0.3195, -1.4508],\n",
      "         [ 1.0626, -0.1594, -0.7191,  0.0479,  2.0683,  2.2863,  0.9569,\n",
      "          -0.9163, -0.3195, -1.4508]],\n",
      "\n",
      "        [[ 0.3910, -1.0598, -1.5889,  0.8460,  0.2837,  0.5221, -0.4213,\n",
      "           1.9279,  0.2396,  0.8146],\n",
      "         [ 1.2032, -0.2405, -0.6877, -1.7124,  0.0894,  1.1393, -0.0744,\n",
      "           0.3660, -0.7260, -0.5404],\n",
      "         [ 1.1101, -1.7800,  0.5034,  2.5597,  1.3629, -0.6775,  0.5924,\n",
      "           0.5835,  0.1828,  1.9872],\n",
      "         [-0.1678, -0.8599, -1.5330, -0.7947,  0.5339, -1.1422, -1.0213,\n",
      "          -0.6911, -0.7178, -0.9006],\n",
      "         [-0.7859,  2.9240, -0.7009,  0.6288,  0.2006, -0.5263, -1.9920,\n",
      "          -0.1586,  0.2252,  1.1975],\n",
      "         [-0.2479,  0.3788,  0.2671,  0.1700,  1.3747,  0.8482,  0.4285,\n",
      "           0.4579, -0.3571,  2.1722],\n",
      "         [ 0.3910, -1.0598, -1.5889,  0.8460,  0.2837,  0.5221, -0.4213,\n",
      "           1.9279,  0.2396,  0.8146],\n",
      "         [-0.9006, -0.8238, -0.7089,  0.4284, -0.1052, -0.2976, -1.0424,\n",
      "          -1.0888, -0.2185, -1.4200],\n",
      "         [ 0.1363,  1.4900, -0.3765,  0.3178,  0.8688, -0.5480, -0.4353,\n",
      "          -0.5306,  0.0042, -0.9358]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.embedding(x)\n",
    "    \n",
    "\n",
    "embedding_dim = 10\n",
    "\n",
    "print(tokenizer.vocab_size)\n",
    "print(tokens)\n",
    "\n",
    "embedding_layer = EmbeddingLayer(tokenizer.vocab_size, embedding_dim)\n",
    "\n",
    "tokens = torch.Tensor(tokens).long()\n",
    "\n",
    "embeddings = embedding_layer(tokens)\n",
    "\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> The Attention Layer </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 9, 10]), torch.Size([5, 9, 10]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(d_model, 3*d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        q, k, v = self.linear(x).chunk(3, dim=-1)\n",
    "\n",
    "        # attn = softmax(Q @ K.T) V\n",
    "        attn = torch.einsum(\"bnd,bkd->bnk\", q, k)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = attn @ v\n",
    "\n",
    "        return attn\n",
    "    \n",
    "\n",
    "attention_layer = Attention(embedding_dim)\n",
    "attn_logits = attention_layer(embeddings)\n",
    "embeddings.shape, attn_logits.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
