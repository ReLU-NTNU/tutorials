{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will explore word embeddings. This allows us to go from words (represented bu string), to vectors, which are easier to handle for machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "!pip install gensim scikit-learn matplotlib numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now loading a pre-trained model containing word-embeddings. This might take some time (~1 minute). While it is loading, feel free to start reading the text below or more about the [word2vec model](https://medium.com/@manansuri/a-dummys-guide-to-word2vec-456444f3c673)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is trained on a large dataset of news articles. Each word is represented by a vector. It is trying to  capture whether words appear in similar contexts or not. For instance, in the sentence \"My favorite drink is [BLANK] juice\", one might expect words such as \"apple\", \"orange\" or \"pineapple\". Since these words often appear in similar contexts, we want their vectors, the *embeddings*, to be similar as well. For other words that would not appear in the same context, such as \"jumping\", \"car\" or \"Germany\", we want the embeddings to be different. A simple way to check how similar to words are, is by looking at their cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar words = similar vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similarity(word1, word2):\n",
    "    print(f\"Similarity between {word1 : ^10} and {word2 : ^10}: {model.similarity(word1, word2):.2f}\")\n",
    "\n",
    "print_similarity(\"apple\", \"orange\")\n",
    "print_similarity(\"apple\", \"pineapple\")\n",
    "print_similarity(\"apple\", \"jumping\")\n",
    "print_similarity(\"apple\", \"car\")\n",
    "print_similarity(\"apple\", \"germany\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all words are just represented by vectors, we can also add or subtract them (and even multiply). Even though the model is very simple, it seems to have captured some understanding of language and abstract concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by the famous example:\n",
    "\n",
    "*king* - *man* + *woman* = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this might seem trivial (and in the age of LLMs, it really is), this was all the hype in 2014. Play around and see if you can find any other funny examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = []     # [\"king\", \"woman\", ..]\n",
    "negative_words = []     # [\"man\", ...]\n",
    "n_words = 3\n",
    "\n",
    "model.most_similar(positive=positive_words, negative=negative_words, topn=n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting the words in 2D (using PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_words(words):\n",
    "    X = [model[word] for word in words]\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    pca = PCA(n_components=2)\n",
    "    result = pca.fit_transform(X)\n",
    "    result = StandardScaler().fit_transform(result)\n",
    "    plt.quiver([0] * len(words), [0] * len(words), result[:, 0], result[:, 1], angles='xy', scale_units='xy', scale=1, color='r')\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "    plt.xlim(-3, 3)\n",
    "    plt.ylim(-3, 3)\n",
    "    plt.title(\"Word Embeddings (projected to 2D)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_words([\"apple\", \"orange\", \"car\", \"germany\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used [plt.quiver(X, Y, U, V)](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.quiver.html) to plot our vectors. Here, (X, Y) is the starting location and (U, V) is the direction of our vector. Look at the code and and try to understand what happens, then run it and look at the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "words = [\"king\", \"queen\", \"man\", \"woman\"]\n",
    "X = [model[word] for word in words]\n",
    "X = StandardScaler().fit_transform(X)\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "result = StandardScaler().fit_transform(result)\n",
    "\n",
    "ORIGO = np.array([0, 0])\n",
    "KING, QUEEN, MAN, WOMAN = result\n",
    "\n",
    "# Plot from origo to king\n",
    "ORIGO_TO_KING = np.concatenate([ORIGO, KING])\n",
    "plt.quiver(*ORIGO_TO_KING, angles='xy', scale_units='xy', scale=1, color='r')\n",
    "\n",
    "# Plot from origo to queen\n",
    "ORIGO_TO_QUEEN = np.concatenate([ORIGO, QUEEN])\n",
    "plt.quiver(*ORIGO_TO_QUEEN, angles='xy', scale_units='xy', scale=1, color='b')\n",
    "\n",
    "# Plot from king to king - man\n",
    "KING_TO_NEG_MAN = np.concatenate([KING, -MAN])\n",
    "print(KING_TO_NEG_MAN)\n",
    "neg_man = -1 * MAN\n",
    "plt.quiver(*KING_TO_NEG_MAN, angles='xy', scale_units='xy', scale=1, color='g')\n",
    "\n",
    "# Plot from king - man to king - man + woman\n",
    "KING_TO_NEG_MAN_POS_WOMAN = np.concatenate([KING-MAN, WOMAN])\n",
    "plt.quiver(*KING_TO_NEG_MAN_POS_WOMAN, angles='xy', scale_units='xy', scale=1, color='y')\n",
    "\n",
    "plt.annotate(\"king\", xy=(result[0, 0], result[0, 1]))\n",
    "plt.annotate(\"queen\", xy=(result[1, 0], result[1, 1]))\n",
    "plt.annotate(\"king - man\", xy=KING-MAN)\n",
    "plt.annotate(\"king - man + queen\", xy=KING-MAN+WOMAN)\n",
    "\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.title(\"Word Embeddings (projected to 2D)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "start-code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
