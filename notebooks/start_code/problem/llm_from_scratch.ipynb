{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Tokenization </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to tokenize text. For simplicity, we use the pretrained GPT-2 tokenizer. This is a byte-level BPE tokenizer. If you're interested in how it works, you can read more [here](https://huggingface.co/learn/nlp-course/en/chapter6/5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hello, my dog is cute\n",
      "Tokens: [15496, 11, 616, 3290, 318, 13779]\n",
      "Decoded: Hello, my dog is cute\n",
      "\n",
      "Text: Hello, my cat is cute\n",
      "Tokens: [15496, 11, 616, 3797, 318, 13779]\n",
      "Decoded: Hello, my cat is cute\n",
      "\n",
      "Text: What is your name?\n",
      "Tokens: [2061, 318, 534, 1438, 30]\n",
      "Decoded: What is your name?\n",
      "\n",
      "Text: My name is John\n",
      "Tokens: [3666, 1438, 318, 1757]\n",
      "Decoded: My name is John\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")   # we load the GPT-2 tokenizer\n",
    "\n",
    "texts = [\n",
    "    \"Hello, my dog is cute\",\n",
    "    \"Hello, my cat is cute\",\n",
    "    \"What is your name?\",\n",
    "    \"My name is John\",\n",
    "]\n",
    "\n",
    "tokens = [tokenizer.encode(text) for text in texts] # we encode the texts\n",
    "\n",
    "for text, token_list in zip(texts, tokens):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Tokens: {token_list}\")\n",
    "    print(f\"Decoded: {tokenizer.decode(token_list)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like our tokenizer is working. The initial texts and the decoded texts are similar. Furthermore, we can see that all the tokenizer is doing is mapping a text to a list of integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Embedding Layer </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # we define the embedding layer w/ input size = vocab_size and output size = embedding_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.embedding(x)\n",
    "    \n",
    "\n",
    "embedding_dim = 10\n",
    "\n",
    "print(tokenizer.vocab_size)\n",
    "print(tokens)\n",
    "\n",
    "embedding_layer = EmbeddingLayer(tokenizer.vocab_size, embedding_dim)\n",
    "\n",
    "tokens = torch.Tensor(tokens).long()\n",
    "\n",
    "embeddings = embedding_layer(tokens)\n",
    "\n",
    "print(embeddings.shape)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now gone from words -> tokens -> embeddings (a vector for each token). Let's get to the meat of the transformer, the *Attention*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> The Attention Layer </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_k):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.linear = nn.Linear(d_model, 3 * d_k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # split the input into Q, K, V\n",
    "        q, k, v = self.linear(x).chunk(3, dim=-1) # we pass our input through a linear layer and then split it into 3 parts\n",
    "\n",
    "        # Recall the formula for the attention mechanism\n",
    "        # attn = softmax(Q K.T / sqrt(d_k)) V\n",
    "        # Hint: Use torch.matmul() for matrix multiplication (https://pytorch.org/docs/stable/generated/torch.matmul.html)\n",
    "        # Hint: Use F.softmax() to apply the softmax function (https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html)\n",
    "        # Hint: Look at the transpose function for PyTorch tensors (https://pytorch.org/docs/stable/generated/torch.transpose.html)\n",
    "        ### YOUR CODE HERE\n",
    "        attn = ...      # multiply Q and K.T\n",
    "        attn = ...      # apply softmax and divide by sqrt(d_k)\n",
    "        attn = ...      # multiply by V\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        return attn\n",
    "    \n",
    "attention_layer = Attention(embedding_dim, embedding_dim)\n",
    "attn_logits = attention_layer(embeddings)\n",
    "\n",
    "# Ensure the shape is the same both before and after the attention layer\n",
    "embeddings.shape, attn_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive implementation of multi-head attention\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // n_heads\n",
    "\n",
    "        # Implement multi-head attention using the Attention module\n",
    "        # Hint: Use nn.ModuleList to hold multiple instances of the Attention module\n",
    "        # Ex: self.heads = nn.ModuleList([AttentionHead1, AttentionHead2, ...])\n",
    "        ### YOUR CODE HERE ###\n",
    "        self.heads = ...\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pass the input through all the heads and concatenate the results\n",
    "        # Hint: Use torch.cat() to concatenate the results of the different heads, specify the dimension using the dim argument\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "        return ...\n",
    "        ### END YOUR CODE ###\n",
    "    \n",
    "d_model = 32\n",
    "n_heads = 4\n",
    "seq_len = 16\n",
    "batch_size = 8\n",
    "shifted_x = torch.randn(batch_size, seq_len, d_model)\n",
    "multi_head_attn = MultiHeadAttention(d_model, n_heads)\n",
    "attn_logits = multi_head_attn(shifted_x)\n",
    "attn_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "        self.norm = ...     # LayerNorm\n",
    "        self.attn = ...     # MultiHeadAttention\n",
    "        self.dropout = ...  # Dropout\n",
    "        self.norm2 = ...    # LayerNorm\n",
    "        self.linear = ...   # Linear layer or multiple linear layers\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        # we first normalize the input\n",
    "        x_attn = self.norm(x)\n",
    "        # we then pass it through the multi-head attention layer and apply dropout\n",
    "        x_attn = self.dropout(self.attn(x_attn))\n",
    "        # we add the input to the output of the multi-head attention\n",
    "        # this is called a residual connection (https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55)\n",
    "        x = x + x_attn\n",
    "\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "        # we pass the output through a linear layer and apply dropout\n",
    "        x_linear = ...\n",
    "        # we apply normalization (remember to use self.norm2, not self.norm)\n",
    "        x_linear = ...\n",
    "        # we add the input to the output of the linear layer\n",
    "        x = ...\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, block_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = ...    # Embedding layer for encoding the input tokens\n",
    "        self.pos_embedding = ... # Positional encoding\n",
    "        self.attention_blocks = ... # Stack of n_layers attention blocks. Hint: use nn.Sequential (https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)\n",
    "        self.fc = ... # Final fully connected layer projecting the model output to the vocab size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x size: (batch_size, seq_len)\n",
    "        # For simplicity, we have implemented the embedding and positional encoding for you\n",
    "        x = self.embedding(x) # (batch_size, seq_len, d_model)\n",
    "        x = x + self.pos_embedding(torch.arange(x.size(1), device=x.device))\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "        x = ...     # pass the input through the stack of attention blocks\n",
    "        x = ...     # pass the output through the final fully connected layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, block_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # we initialize the transformer model we created\n",
    "        self.transformer = Transformer(vocab_size, d_model, n_heads, n_layers, block_size)\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "        self.loss_fn = ... # Loss function for training the model\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        logits = self.transformer(x)    # we pass the input through the transformer\n",
    "        loss = None\n",
    "        if targets is not None:         # if we have targets, we calculate the loss\n",
    "            loss = self.loss_fn(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss     # we return the logits and the loss\n",
    "\n",
    "    def generate(self, x, steps=100, deterministic=False):\n",
    "        # we generate text by passing the input through the transformer model repeatedly\n",
    "        for _ in range(steps):\n",
    "            logits = self.transformer(x)    # we pass the input through the transformer\n",
    "            last_token_logits = logits[:, -1]   # we get the probabilty distribution of the last token\n",
    "            if deterministic:   # if we are in deterministic mode, we take the token with the highest probability\n",
    "                next_token = torch.argmax(last_token_logits, dim=-1).unsqueeze(-1)\n",
    "            else:  # otherwise, we sample from the probability distribution\n",
    "                next_token = torch.multinomial(F.softmax(last_token_logits, dim=-1), num_samples=1)\n",
    "            x = torch.cat([x, next_token], dim=-1)  # we concatenate the next token to the input\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PATH_TO_TEXT_FILE = Path.cwd().parent / \"data\" / \"input.txt\" # we define the path to the text file\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(text, block_size):\n",
    "\n",
    "    tokens = tokenizer.encode(text)\n",
    "\n",
    "    for i in range(0, len(tokens) - block_size, block_size):\n",
    "        yield tokens[i:i+block_size], tokens[i+1:i+block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Define the hyperparameters for training\n",
    "# In general, fewer epochs means faster training, but the model may not have enough time to learn\n",
    "# A larger block size means the model can learn more context, but training will be slower\n",
    "# A larger d_model, n_heads, and n_layers means the model can learn more complex patterns, but training will be slower\n",
    "\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "num_epochs = 10     # Number of epochs to train the model, you can change this\n",
    "block_size = 256    # Length of the sequence to train the model on, you can change this (try 128, 256, 512)\n",
    "d_model = 256       # Dimension of the model, you can change this\n",
    "n_heads = 4         # Number of attention heads, you can change this\n",
    "n_layers = 4        # Number of transformer layers, you can change this\n",
    "lr = 1e-4           # Learning rate for training, you can change this (try 1e-3, 1e-4, 1e-5)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = GPT(tokenizer.vocab_size, d_model, n_heads, n_layers, block_size).to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tqdm(get_batch(text, block_size), desc=f\"Training epoch {epoch+1}\", total=len(tokenizer.encode(text))//block_size):\n",
    "        x, y = torch.tensor(batch[0]).unsqueeze(0).to(device), torch.tensor(batch[1]).unsqueeze(0).to(device)\n",
    "        logits, loss = model(x, y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "context = \"PROPSERO:\"       # The starting text for generation, you can change this or leave it empty\n",
    "determinstic = False        # Set this to True for deterministic generation, or False for stochastic generation\n",
    "### END YOUR CODE ###\n",
    "\n",
    "if context:\n",
    "    x = torch.tensor(tokenizer.encode(context)).unsqueeze(0).to(device)\n",
    "else:\n",
    "    x = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "output = model.generate(x, deterministic=determinstic)\n",
    "print(tokenizer.decode(output[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "start-code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
