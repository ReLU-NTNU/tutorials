{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Word Embeddings </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Tokenization </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'am', 'brown', 'doctor', 'dog', 'fox', 'i', 'jumps', 'lazy', 'over', 'programmer', 'quick', 'student', 'teacher', 'the']\n",
      "[[6, 1, 0, 12, 15, 15, 15, 15, 15], [6, 1, 0, 13, 15, 15, 15, 15, 15], [6, 1, 0, 3, 15, 15, 15, 15, 15], [6, 1, 0, 10, 15, 15, 15, 15, 15], [14, 11, 2, 5, 7, 9, 14, 8, 4]]\n",
      "9 [9, 9, 9, 9, 9]\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.vocab = None\n",
    "        self.max_len = None\n",
    "        self.vocab_size = None\n",
    "\n",
    "    def preprocess(self, text):\n",
    "\n",
    "        # remove all non alphabetic characters\n",
    "        text = ''.join(e for e in text if e.isalnum() or e.isspace())\n",
    "\n",
    "        # convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        return text\n",
    "\n",
    "    def generate_vocab(self, texts):\n",
    "\n",
    "        text_preprocessed = [self.preprocess(text) for text in texts]\n",
    "\n",
    "        self.max_len = max([len(seq.split()) for seq in text_preprocessed])\n",
    "\n",
    "        words = \" \".join(text_preprocessed).split()\n",
    "\n",
    "        vocab = list(set(words))\n",
    "\n",
    "        vocab.sort()\n",
    "\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.vocab_size = len(self.vocab) + 1\n",
    "    \n",
    "    def tokenize(self, texts):\n",
    "\n",
    "        total_tokens = []\n",
    "\n",
    "        for text in texts:\n",
    "    \n",
    "            text_preprocessed = self.preprocess(text)\n",
    "\n",
    "            words = text_preprocessed.split()\n",
    "\n",
    "            tokens = []\n",
    "\n",
    "            for word in words:\n",
    "                tokens.append(self.vocab.index(word))\n",
    "            \n",
    "            tokens += [self.vocab_size-1 for _ in range(self.max_len - len(tokens))]\n",
    "\n",
    "            total_tokens.append(tokens)\n",
    "\n",
    "        return total_tokens\n",
    "\n",
    "texts = [\n",
    "    \"I am a student\", \n",
    "    \"I am a teacher\", \n",
    "    \"I am a doctor\", \n",
    "    \"I am a programmer\", \n",
    "    \"The quick brown fox jumps over the lazy dog\"\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.generate_vocab(texts)\n",
    "\n",
    "print(tokenizer.vocab)\n",
    "\n",
    "tokens = tokenizer.tokenize(texts)\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "print(tokenizer.max_len, [len(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Embedding Layer </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "[[6, 1, 0, 12, 15, 15, 15, 15, 15], [6, 1, 0, 13, 15, 15, 15, 15, 15], [6, 1, 0, 3, 15, 15, 15, 15, 15], [6, 1, 0, 10, 15, 15, 15, 15, 15], [14, 11, 2, 5, 7, 9, 14, 8, 4]]\n",
      "torch.Size([5, 9, 10])\n",
      "tensor([[[ 0.6797,  0.0868, -0.7239,  0.4557,  0.6637, -0.7207, -0.0239,\n",
      "           0.4902,  0.2609, -0.5700],\n",
      "         [ 0.3847, -1.1373,  1.0454,  0.3670, -0.1145,  1.9662,  1.3524,\n",
      "           0.8335, -0.1486, -0.0880],\n",
      "         [-0.2736,  0.7379, -1.4303,  0.6544, -0.0867, -0.4270,  0.8674,\n",
      "           0.5018,  0.8120,  0.9697],\n",
      "         [-0.8729,  2.0364, -1.2143, -1.0850, -0.0095,  1.3176, -0.1300,\n",
      "           0.8922, -0.1097, -0.8783],\n",
      "         [-0.1839,  0.2155, -0.4136, -1.6557,  0.7286, -1.0491,  0.0059,\n",
      "          -0.0436, -1.4449,  0.2166],\n",
      "         [-0.1839,  0.2155, -0.4136, -1.6557,  0.7286, -1.0491,  0.0059,\n",
      "          -0.0436, -1.4449,  0.2166],\n",
      "         [-0.1839,  0.2155, -0.4136, -1.6557,  0.7286, -1.0491,  0.0059,\n",
      "          -0.0436, -1.4449,  0.2166],\n",
      "         [-0.1839,  0.2155, -0.4136, -1.6557,  0.7286, -1.0491,  0.0059,\n",
      "          -0.0436, -1.4449,  0.2166],\n",
      "         [-0.1839,  0.2155, -0.4136, -1.6557,  0.7286, -1.0491,  0.0059,\n",
      "          -0.0436, -1.4449,  0.2166]],\n",
      "\n",
      "        [[ 0.6797,  0.0868, -0.7239,  0.4557,  0.6637, -0.7207, -0.0239,\n",
      "           0.4902,  0.2609, -0.5700],\n",
      "         [ 0.3847, -1.1373,  1.0454,  0.3670, -0.1145,  1.9662,  1.3524,\n",
      "           0.8335, -0.1486, -0.0880],\n",
      "         [-0.2736,  0.7379, -1.4303,  0.6544, -0.0867, -0.4270,  0.8674,\n",
      "           0.5018,  0.8120,  0.9697],\n",
      "         [-2.4090,  0.6954,  0.6683,  0.2232, -1.2783, -0.2970, -0.5397,\n",
      "           0.8571,  0.2159, -0.3090],\n",
      "         [-0.1839,  0.2155, -0.4136, -1.6557,  0.7286, -1.0491,  0.0059,\n",
      "          -0.0436, -1.4449,  0.2166],\n",
      "         [-0.1839,  0.2155, -0.4136, -1.6557,  0.7286, -1.0491,  0.0059,\n",
      "          -0.0436, -1.4449,  0.2166],\n",
      "         [-0.1839,  0.2155, -0.4136, -1.6557,  0.7286, -1.0491,  0.0059,\n",
      "          -0.0436, -1.4449,  0.2166],\n",
      "         [-0.1839,  0.2155, -0.4136, -1.6557,  0.7286, -1.0491,  0.0059,\n",
      "          -0.0436, -1.4449,  0.2166],\n",
      "         [-0.1839,  0.2155, -0.4136, -1.6557,  0.7286, -1.0491,  0.0059,\n",
      "          -0.0436, -1.4449,  0.2166]],\n",
      "\n",
      "        [[ 0.6797,  0.0868, -0.7239,  0.4557,  0.6637, -0.7207, -0.0239,\n",
      "           0.4902,  0.2609, -0.5700],\n",
      "         [ 0.3847, -1.1373,  1.0454,  0.3670, -0.1145,  1.9662,  1.3524,\n",
      "           0.8335, -0.1486, -0.0880],\n",
      "         [-0.2736,  0.7379, -1.4303,  0.6544, -0.0867, -0.4270,  0.8674,\n",
      "           0.5018,  0.8120,  0.9697],\n",
      "         [-1.4829,  1.5810,  0.1708, -0.3741,  1.6249,  0.3159,  2.4048,\n",
      "           2.0498, -0.6386,  1.0441],\n",
      "         [-0.1839,  0.2155, -0.4136, -1.6557,  0.7286, -1.0491,  0.0059,\n",
      "          -0.0436, -1.4449,  0.2166],\n",
      "         [-0.1839,  0.2155, -0.4136, -1.6557,  0.7286, -1.0491,  0.0059,\n",
      "          -0.0436, -1.4449,  0.2166],\n",
      "         [-0.1839,  0.2155, -0.4136, -1.6557,  0.7286, -1.0491,  0.0059,\n",
      "          -0.0436, -1.4449,  0.2166],\n",
      "         [-0.1839,  0.2155, -0.4136, -1.6557,  0.7286, -1.0491,  0.0059,\n",
      "          -0.0436, -1.4449,  0.2166],\n",
      "         [-0.1839,  0.2155, -0.4136, -1.6557,  0.7286, -1.0491,  0.0059,\n",
      "          -0.0436, -1.4449,  0.2166]],\n",
      "\n",
      "        [[ 0.6797,  0.0868, -0.7239,  0.4557,  0.6637, -0.7207, -0.0239,\n",
      "           0.4902,  0.2609, -0.5700],\n",
      "         [ 0.3847, -1.1373,  1.0454,  0.3670, -0.1145,  1.9662,  1.3524,\n",
      "           0.8335, -0.1486, -0.0880],\n",
      "         [-0.2736,  0.7379, -1.4303,  0.6544, -0.0867, -0.4270,  0.8674,\n",
      "           0.5018,  0.8120,  0.9697],\n",
      "         [-0.3118,  0.1186, -2.1442, -1.0995,  2.4190, -0.4023,  0.4120,\n",
      "           0.3770,  0.4224, -0.4343],\n",
      "         [-0.1839,  0.2155, -0.4136, -1.6557,  0.7286, -1.0491,  0.0059,\n",
      "          -0.0436, -1.4449,  0.2166],\n",
      "         [-0.1839,  0.2155, -0.4136, -1.6557,  0.7286, -1.0491,  0.0059,\n",
      "          -0.0436, -1.4449,  0.2166],\n",
      "         [-0.1839,  0.2155, -0.4136, -1.6557,  0.7286, -1.0491,  0.0059,\n",
      "          -0.0436, -1.4449,  0.2166],\n",
      "         [-0.1839,  0.2155, -0.4136, -1.6557,  0.7286, -1.0491,  0.0059,\n",
      "          -0.0436, -1.4449,  0.2166],\n",
      "         [-0.1839,  0.2155, -0.4136, -1.6557,  0.7286, -1.0491,  0.0059,\n",
      "          -0.0436, -1.4449,  0.2166]],\n",
      "\n",
      "        [[-1.5260, -0.4228, -0.3455,  1.4225, -0.7181,  0.3957,  0.6123,\n",
      "           0.7075, -0.2928, -0.4179],\n",
      "         [ 1.8275, -1.0241,  0.7626, -1.1333, -0.4772,  0.5512, -0.6832,\n",
      "           1.9794, -2.4161, -0.1594],\n",
      "         [-0.4303, -1.5053, -0.1814, -1.0825, -0.5550,  0.7065, -0.1966,\n",
      "           0.5711,  1.0041, -0.7723],\n",
      "         [ 1.2845,  1.6161,  0.9871, -1.6479, -0.4879,  0.7717, -0.1778,\n",
      "           0.7682, -0.0043, -0.1461],\n",
      "         [ 0.3801, -1.1374,  0.5904, -0.3921, -0.6162, -1.4038, -0.2024,\n",
      "          -1.5973,  0.4270, -0.0821],\n",
      "         [ 0.3999,  0.4725,  0.8114, -1.1068,  0.6157, -0.0331, -0.3720,\n",
      "           0.1202, -0.7249, -0.2237],\n",
      "         [-1.5260, -0.4228, -0.3455,  1.4225, -0.7181,  0.3957,  0.6123,\n",
      "           0.7075, -0.2928, -0.4179],\n",
      "         [ 0.2714, -0.4894, -0.7009,  0.8106, -0.7358,  0.8287,  2.0496,\n",
      "          -0.8993,  0.2472,  0.0223],\n",
      "         [ 0.1293, -0.5153,  0.8053,  0.0182, -1.5997, -1.4911,  0.1104,\n",
      "          -0.3596, -0.2251,  0.3748]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.embedding(x)\n",
    "    \n",
    "\n",
    "embedding_dim = 10\n",
    "\n",
    "print(tokenizer.vocab_size)\n",
    "print(tokens)\n",
    "\n",
    "embedding_layer = EmbeddingLayer(tokenizer.vocab_size, embedding_dim)\n",
    "\n",
    "tokens = torch.Tensor(tokens).long()\n",
    "\n",
    "embeddings = embedding_layer(tokens)\n",
    "\n",
    "print(embeddings.shape)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> The Attention Layer </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 9, 10]), torch.Size([5, 9, 10]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, 3 * head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # split the input into Q, K, V\n",
    "        q, k, v = self.linear(x).chunk(3, dim=-1) # we pass our input through a linear layer and then split it into 3 parts\n",
    "\n",
    "        # Recall the formula for the attention mechanism\n",
    "        # attn = softmax(Q K.T) V\n",
    "        # Hint: Use torch.matmul() for matrix multiplication (https://pytorch.org/docs/stable/generated/torch.matmul.html)\n",
    "        # Hint: Use F.softmax() to apply the softmax function (https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html)\n",
    "        # Hint: Look at the transpose function for PyTorch tensors (https://pytorch.org/docs/stable/generated/torch.transpose.html)\n",
    "        ### YOUR CODE HERE\n",
    "        attn = ...\n",
    "\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        return attn\n",
    "    \n",
    "attention_layer = Attention(embedding_dim, embedding_dim)\n",
    "attn_logits = attention_layer(embeddings)\n",
    "\n",
    "# Ensure the shape is the same both before and after the attention layer\n",
    "embeddings.shape, attn_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# naive implementation of multi-head attention\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_size = d_model // n_heads\n",
    "\n",
    "        # Implement multi-head attention using the Attention module\n",
    "        # Hint: Use nn.ModuleList to hold multiple instances of the Attention module\n",
    "        # Ex: self.heads = nn.ModuleList([AttentionHead1, AttentionHead2, ...])\n",
    "        ### YOUR CODE HERE ###\n",
    "        self.heads = ...\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pass the input through all the heads and concatenate the results\n",
    "        # Hint: Use torch.cat() to concatenate the results of the different heads, specify the dimension using the dim argument\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "        return ...\n",
    "        ### END YOUR CODE ###\n",
    "    \n",
    "d_model = 32\n",
    "n_heads = 4\n",
    "seq_len = 16\n",
    "batch_size = 8\n",
    "shifted_x = torch.randn(batch_size, seq_len, d_model)\n",
    "multi_head_attn = MultiHeadAttention(d_model, n_heads)\n",
    "attn_logits = multi_head_attn(shifted_x)\n",
    "attn_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "        self.norm = ...     # LayerNorm\n",
    "        self.attn = ...     # MultiHeadAttention\n",
    "        self.dropout = ...  # Dropout\n",
    "        self.norm2 = ...    # LayerNorm\n",
    "        self.linear = ...   # Linear layer or multiple linear layers\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        # we first normalize the input\n",
    "        x_attn = self.norm(x)\n",
    "        # we then pass it through the multi-head attention layer and apply dropout\n",
    "        x_attn = self.dropout(self.attn(x_attn))\n",
    "        # we add the input to the output of the multi-head attention\n",
    "        # this is called a residual connection (https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55)\n",
    "        x = x + x_attn\n",
    "\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "        # we pass the output through a linear layer and apply dropout\n",
    "        x_linear = ...\n",
    "        # we apply normalization (remember to use self.norm2, not self.norm)\n",
    "        x_linear = ...\n",
    "        # we add the input to the output of the linear layer\n",
    "        x = ...\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, block_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = ...    # Embedding layer for encoding the input tokens\n",
    "        self.pos_embedding = ... # Positional encoding\n",
    "        self.attention_blocks = ... # Stack of n_layers attention blocks. Hint: use nn.Sequential (https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)\n",
    "        self.fc = ... # Final fully connected layer projecting the model output to the vocab size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x size: (batch_size, seq_len)\n",
    "        # For simplicity, we have implemented the embedding and positional encoding for you\n",
    "        x = self.embedding(x) # (batch_size, seq_len, d_model)\n",
    "        x = x + self.pos_embedding(torch.arange(x.size(1), device=x.device))\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "        x = ...     # pass the input through the stack of attention blocks\n",
    "        x = ...     # pass the output through the final fully connected layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, block_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer = Transformer(vocab_size, d_model, n_heads, n_layers, block_size)\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "        self.loss_fn = ... # Loss function for training the model\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        logits = self.transformer(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = self.loss_fn(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x, steps=100, deterministic=False):\n",
    "        for _ in range(steps):\n",
    "            logits = self.transformer(x)\n",
    "            last_token_logits = logits[:, -1]\n",
    "            if deterministic:\n",
    "                next_token = torch.argmax(last_token_logits, dim=-1).unsqueeze(-1)\n",
    "            else:\n",
    "                next_token = torch.multinomial(F.softmax(last_token_logits, dim=-1), num_samples=1)\n",
    "            x = torch.cat([x, next_token], dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oskarjor/miniconda3/envs/start-code/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(text, block_size):\n",
    "\n",
    "    tokens = tokenizer.encode(text)\n",
    "\n",
    "    for i in range(0, len(tokens) - block_size, block_size):\n",
    "        yield tokens[i:i+block_size], tokens[i+1:i+block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (338025 > 1024). Running this sequence through the model will result in indexing errors\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training epoch 1: 100%|██████████| 1320/1320 [01:03<00:00, 20.79it/s]\n",
      "Training epoch 2: 100%|██████████| 1320/1320 [01:01<00:00, 21.53it/s]\n",
      "Training epoch 3: 100%|██████████| 1320/1320 [00:59<00:00, 22.30it/s]\n",
      "Training epoch 4: 100%|██████████| 1320/1320 [00:59<00:00, 22.30it/s]\n",
      "Training epoch 5: 100%|██████████| 1320/1320 [00:57<00:00, 23.15it/s]\n",
      "Training epoch 6: 100%|██████████| 1320/1320 [00:56<00:00, 23.21it/s]\n",
      "Training epoch 7: 100%|██████████| 1320/1320 [00:57<00:00, 23.13it/s]\n",
      "Training epoch 8: 100%|██████████| 1320/1320 [00:57<00:00, 23.05it/s]\n",
      "Training epoch 9: 100%|██████████| 1320/1320 [00:56<00:00, 23.17it/s]\n",
      "Training epoch 10: 100%|██████████| 1320/1320 [00:58<00:00, 22.56it/s]\n",
      "Training epoch 11: 100%|██████████| 1320/1320 [00:58<00:00, 22.63it/s]\n",
      "Training epoch 12: 100%|██████████| 1320/1320 [00:57<00:00, 22.78it/s]\n",
      "Training epoch 13: 100%|██████████| 1320/1320 [00:58<00:00, 22.65it/s]\n",
      "Training epoch 14: 100%|██████████| 1320/1320 [00:58<00:00, 22.42it/s]\n",
      "Training epoch 15: 100%|██████████| 1320/1320 [00:58<00:00, 22.66it/s]\n",
      "Training epoch 16: 100%|██████████| 1320/1320 [01:00<00:00, 21.71it/s]\n",
      "Training epoch 17: 100%|██████████| 1320/1320 [00:57<00:00, 23.00it/s]\n",
      "Training epoch 18: 100%|██████████| 1320/1320 [00:57<00:00, 23.04it/s]\n",
      "Training epoch 19: 100%|██████████| 1320/1320 [06:56<00:00,  3.17it/s]\n",
      "Training epoch 20: 100%|██████████| 1320/1320 [02:34<00:00,  8.52it/s]\n",
      "Training epoch 21: 100%|██████████| 1320/1320 [57:46<00:00,  2.63s/it]   \n",
      "Training epoch 22: 100%|██████████| 1320/1320 [01:01<00:00, 21.52it/s]\n",
      "Training epoch 23: 100%|██████████| 1320/1320 [00:58<00:00, 22.52it/s]\n",
      "Training epoch 24: 100%|██████████| 1320/1320 [00:58<00:00, 22.68it/s]\n",
      "Training epoch 25: 100%|██████████| 1320/1320 [00:58<00:00, 22.40it/s]\n",
      "Training epoch 26: 100%|██████████| 1320/1320 [01:00<00:00, 21.80it/s]\n",
      "Training epoch 27: 100%|██████████| 1320/1320 [01:09<00:00, 18.95it/s]\n",
      "Training epoch 28: 100%|██████████| 1320/1320 [01:14<00:00, 17.65it/s]\n",
      "Training epoch 29: 100%|██████████| 1320/1320 [01:12<00:00, 18.23it/s]\n",
      "Training epoch 30: 100%|██████████| 1320/1320 [01:02<00:00, 21.19it/s]\n",
      "Training epoch 31: 100%|██████████| 1320/1320 [00:59<00:00, 22.01it/s]\n",
      "Training epoch 32: 100%|██████████| 1320/1320 [00:58<00:00, 22.57it/s]\n",
      "Training epoch 33: 100%|██████████| 1320/1320 [00:57<00:00, 22.81it/s]\n",
      "Training epoch 34: 100%|██████████| 1320/1320 [00:58<00:00, 22.64it/s]\n",
      "Training epoch 35: 100%|██████████| 1320/1320 [00:58<00:00, 22.48it/s]\n",
      "Training epoch 36: 100%|██████████| 1320/1320 [00:59<00:00, 22.04it/s]\n",
      "Training epoch 37: 100%|██████████| 1320/1320 [00:59<00:00, 22.10it/s]\n",
      "Training epoch 38: 100%|██████████| 1320/1320 [00:59<00:00, 22.00it/s]\n",
      "Training epoch 39: 100%|██████████| 1320/1320 [00:58<00:00, 22.70it/s]\n",
      "Training epoch 40: 100%|██████████| 1320/1320 [00:58<00:00, 22.73it/s]\n",
      "Training epoch 41: 100%|██████████| 1320/1320 [00:57<00:00, 22.84it/s]\n",
      "Training epoch 42: 100%|██████████| 1320/1320 [00:58<00:00, 22.54it/s]\n",
      "Training epoch 43: 100%|██████████| 1320/1320 [00:58<00:00, 22.50it/s]\n",
      "Training epoch 44: 100%|██████████| 1320/1320 [00:58<00:00, 22.67it/s]\n",
      "Training epoch 45: 100%|██████████| 1320/1320 [00:57<00:00, 22.99it/s]\n",
      "Training epoch 46: 100%|██████████| 1320/1320 [00:57<00:00, 22.85it/s]\n",
      "Training epoch 47: 100%|██████████| 1320/1320 [00:57<00:00, 22.82it/s]\n",
      "Training epoch 48: 100%|██████████| 1320/1320 [00:58<00:00, 22.69it/s]\n",
      "Training epoch 49: 100%|██████████| 1320/1320 [00:57<00:00, 22.82it/s]\n",
      "Training epoch 50: 100%|██████████| 1320/1320 [00:57<00:00, 22.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Define the hyperparameters for training\n",
    "# In general, fewer epochs means faster training, but the model may not have enough time to learn\n",
    "# A larger block size means the model can learn more context, but training will be slower\n",
    "# A larger d_model, n_heads, and n_layers means the model can learn more complex patterns, but training will be slower\n",
    "\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "num_epochs = 10     # Number of epochs to train the model, you can change this\n",
    "block_size = 256    # Length of the sequence to train the model on, you can change this (try 128, 256, 512)\n",
    "d_model = 256       # Dimension of the model, you can change this\n",
    "n_heads = 4         # Number of attention heads, you can change this\n",
    "n_layers = 4        # Number of transformer layers, you can change this\n",
    "lr = 1e-4           # Learning rate for training, you can change this (try 1e-3, 1e-4, 1e-5)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = GPT(tokenizer.vocab_size, d_model, n_heads, n_layers, block_size).to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tqdm(get_batch(text, block_size), desc=f\"Training epoch {epoch+1}\", total=len(tokenizer.encode(text))//block_size):\n",
    "        x, y = torch.tensor(batch[0]).unsqueeze(0).to(device), torch.tensor(batch[1]).unsqueeze(0).to(device)\n",
    "        logits, loss = model(x, y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPSERO:\n",
      "Sir when she know, if the more of Juliet's, good lord so thou\n",
      "Of thy father'st prev more than it like of holy\n",
      "First goodness:\n",
      "O no thought\n",
      "But from the bestOLANDA:\n",
      "We'll draw,\n",
      "To those\n",
      "Was ever What.\n",
      "What dogs.\n",
      "She eat'd themselves and upon my ghost in his beard?\n",
      "\n",
      "VIRGILIA:\n",
      "Than to watch:\n",
      "Come on his poor father? what thou\n"
     ]
    }
   ],
   "source": [
    "context = \"PROPSERO:\" # None | str: \"The quick brown fox jumps over the lazy dog\"\n",
    "if context:\n",
    "    x = torch.tensor(tokenizer.encode(context)).unsqueeze(0).to(device)\n",
    "else:\n",
    "    x = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "output = model.generate(x, deterministic=False)\n",
    "print(tokenizer.decode(output[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "start-code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
