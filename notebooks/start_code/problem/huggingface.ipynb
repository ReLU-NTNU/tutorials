{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading a GPT-2 model from Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training a model on your laptop is awesome, it is also very slow (and expensive)! Luckily, people with bigger computers and more money have done this already. In this notebook, you will load the GPT-2 model, trained by OpenAI. This model has 1.5 billion parameters and was trained on ~40 GB of data. And even this model, which is many orders of magnitude larger than the one we implemented, is absolutely tiny compared newer models like GPT-4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image showing the evolution of LLMs in terms of the number of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Size of LLMs](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F91508b45-a749-4afe-95a4-7d4ec3943ed8_1048x588.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by running these cells to install the necessary libraries and load the pre-trained GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now generate some text! Change the parameters and see what the model actually outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "text = \"Once upon a time\"   # change the input text here\n",
    "temperature = 1.0           # change the temperature here. \n",
    "                            # This determines how random the text generation is. \n",
    "                            # Lower values are more deterministic, higher values are more random.\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "input_ids = encoded_input['input_ids']\n",
    "attn_mask = encoded_input['attention_mask']\n",
    "gen_tokens = model.generate(\n",
    "    input_ids, \n",
    "    attention_mask=attn_mask,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=temperature,\n",
    "    max_length=100\n",
    ")\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next-word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the lecture that all we are really doing is trying to predict the probability distribution of the next word (technically we are predicting the next token, and a token might be a sub-word or even several words, but it is easier if we just think about predicting words). How might this probability distribution look? Before you run the code, consider this. Consider a text starting with:\n",
    "\n",
    "<center><h1>Once upon a time...</h1></center>\n",
    "\n",
    "What do you think is the most probable next word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "text = \"Once upon a time\"   # change the input text here. For instance, try \"I am Iron\" and see what happens.\n",
    "temperature = 1.0           # change the temperature here. \n",
    "                            # This determines how random the text generation is. \n",
    "                            # Lower values are more deterministic, higher values are more random.\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "input_ids = encoded_input['input_ids']\n",
    "attn_mask = encoded_input['attention_mask']\n",
    "gen_tokens = model.generate(\n",
    "    input_ids, \n",
    "    attention_mask=attn_mask,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=temperature,\n",
    "    max_length=100\n",
    ")\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "\n",
    "# we will only look at the next token\n",
    "k = 50\n",
    "next_token = model(**encoded_input).logits[0, -1, :].detach()\n",
    "top_k = next_token.topk(k).indices\n",
    "top_k_tokens = tokenizer.batch_decode(top_k)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "ax.barh(top_k_tokens, next_token[top_k].softmax(dim=0))\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Probability')\n",
    "ax.set_title(text + \"...\")\n",
    "ax.title.set_fontsize(40)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "start-code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
