{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Introduction to Computer Vision (mainly convolutional nets) </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of this notebook is to show you how to leverage one of the most powerful ideas in deep learning, namely convolutional neural networks. The state of the art models for virtually all tasks in computer vision are convolutional (although vision transformers have caught up in some cases). These models are also very relevant for deep learning with time series data.\n",
    "\n",
    "The point is to make you comfortable with using convolutional nets as a tool in your machine learning toolbox. It is a very useful tool for any ML engineer or researcher, and they are surprisingly quick to implement when you know the basics.\n",
    "\n",
    "The notebook starts by showing why we need convnets, and why they work. You will then experiment with the different layers usually found in these nets. After that, you will implement the following models:\n",
    "\n",
    "\n",
    "1. Convolutional Net + Multilayer Perceptron for classification\n",
    "2. Autoencoder for compression and unsupervised feature learning\n",
    "3. U-Net for image segmentation\n",
    "4. U-Net for generative diffusion modeling\n",
    "\n",
    "The notebook is quite long and you are not meant to solve everything in one sitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import Caltech101, OxfordIIITPet\n",
    "from torchvision import transforms\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Why convolutional neural networks? </h2>\n",
    "\n",
    "To understand this, we can start by seeing what happens when we use a standard fully connected network to model images. Let us start by fetching some images from Caltech. This is a classification dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Caltech101(\n",
    "    \"data\", \n",
    "    download=True,\n",
    "    transform= transforms.Compose([transforms.ToTensor(), transforms.Resize((256, 256))])\n",
    ")\n",
    "\n",
    "# We retrieve a random sample from the dataset\n",
    "image, _ = data[4000]\n",
    "\n",
    "# We show the image. Note that we need to permute the image so that the shape is \n",
    "# (spatial_dim, spatial_dim, num_channels), instead of (num_channels, spatial_dim, spatial_dim)\n",
    "# that Pytorch works with. All standard image processing libraries require the former shape.\n",
    "plt.imshow(image.permute(1,2,0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> How much data is required to represent the image? </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Overall, one image is represented by num_channels*height*width numbers </h3>\n",
    "\n",
    "To have the image work with a fully connected net, we need to flatten it (since it expects the input as a vector):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_flattened = image.flatten()\n",
    "image_flattened.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Now we define an MLP to work with the flattened images: </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size, hidden_size, num_labels = (196608, 196608//4, 101)\n",
    "\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(image_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, num_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Now we do a standard forward and backward pass with the model: </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into batches\n",
    "dataloader = DataLoader(data, batch_size=32)\n",
    "# Get the first batch\n",
    "images, labels = next(iter(dataloader))\n",
    "# Flatten the images so that they work with the MLP\n",
    "images_flat = images.flatten(start_dim=1)\n",
    "\n",
    "# Forward pass\n",
    "preds = mlp(images_flat)\n",
    "# Cross Entropy is a popular loss function for classification\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# We one hot encode the labels to facilitate loss computation\n",
    "# This means that 4 becomes [0,0,0,0,1,0,0...]\n",
    "y_true = F.one_hot(labels.long(), num_classes=101).float()\n",
    "# Compute the loss\n",
    "loss = loss_fn(y_true, preds)\n",
    "# Backprop\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Your kernel probably crashed, and we conclude that running a MLP on large data such as raw images is a bad idea </h3>\n",
    "\n",
    "Furthermore, MLPs are fundamentally non-sequential, meaning that the architecture does not take advantage of the spatial and sequential nature of images. This, as well as the computational problem that we just explored, motivates the need for an architecture that:\n",
    "\n",
    "1. Avoids needing millions of parameters when working with images\n",
    "2. Takes into account that image features are local (we look at patches to find patterns)\n",
    "3. Takes into account that the same pattern can show up anywhere in an image\n",
    "\n",
    "Convolutional nets solve all the above problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> The Convolutional Layer </h2>\n",
    "\n",
    "Read this note for a concise intro to convolutional nets: https://inst.eecs.berkeley.edu/~cs182/sp23/assets/notes/scribe8.pdf (you can skip it if you feel comfortable with convnets). \n",
    "\n",
    "The main component of a CNN is the convolutional layer. As you read above, this layer consists of numerous filters where you basically take the dot product between the flattened filter and a flattened patch in the input image, and do this for all patches (where you set the patch size, the number of filters, and what patches to look at as the input). Let us now explore this layer in PyTorch.\n",
    "\n",
    "First let's go through the parameters of a convolutional layer:\n",
    "<br/>\n",
    "1. in_channels\n",
    "<br/>\n",
    "<br/>\n",
    "This is just the number of channels in the input. An image has 3 dimensions, num_channels, height, and width. We need to pass the num_channels of the input image.\n",
    "<br/>\n",
    "<br/>\n",
    "2. out_channels\n",
    "<br/>\n",
    "<br/>\n",
    "This defines how many channels the output will have. This is the same as how many different filters we have in the layer (each filter produces one output number, and we want out_channels output numbers per patch. We therefore need out_channels filters)\n",
    "<br/>\n",
    "<br/>\n",
    "3. kernel_size\n",
    "<br/>\n",
    "<br/>\n",
    "This defines the size of the filter, i.e, how large the patches we evaluate will be.\n",
    "<br/>\n",
    "<br/>\n",
    "4. stride\n",
    "<br/>\n",
    "<br/>\n",
    "This decides how far we slide the filter when we move from patch to patch. We can go to the next patch by moving one pixel to the right, or we can jump over some pixels instead. The stride number tells how far we jump (a value of 1 means we just go to the next pixel).\n",
    "<br/>\n",
    "<br/>\n",
    "5. padding\n",
    "<br/>\n",
    "<br/>\n",
    "We can add a frame of 0s to the image, in case we want to control the output shape, or to make sure that we don't miss a feature that appears in a corner or the side of an image. The padding decides how large the frame should be. For example:\n",
    "\n",
    "Padding=0:\n",
    "<br/>\n",
    "X\n",
    "\n",
    "Padding=1:\n",
    "<br/>\n",
    "000\n",
    "<br/>\n",
    "0X0\n",
    "<br/>\n",
    "000\n",
    "\n",
    "Padding=2:\n",
    "<br/>\n",
    "00000\n",
    "<br/>\n",
    "00X00\n",
    "<br/>\n",
    "00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's once again print the shape of our images\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the layer\n",
    "# You can experiment with the parameters if you want\n",
    "# The default is a classic strided convolution that halves the spatial dimension (due to the stride + padding)\n",
    "conv = nn.Conv2d(\n",
    "    in_channels=3, # we set it to 3 since there are 3 channels in the image\n",
    "    out_channels=32, \n",
    "    kernel_size=3, \n",
    "    stride=2,\n",
    "    padding=1\n",
    ")\n",
    "\n",
    "out = conv(images)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Building a convolutional net for image classification </h2>\n",
    "There are other layers that are frequently used with CNNs, and of course standard activation functions such as ReLU are necessary to learn nonlinear relationships here as well. One layer you can look into is the max pooling layer: https://paperswithcode.com/method/max-pooling. The output is just the max in the patch (this layer is actually not used that much in newer models, it's kind of old school). A very important class of layers is the normalization layer, but we will not go deep into the theory now. Batch normalization is a very powerful tool, and we recommend you use it in your models. \n",
    "\n",
    "Now we will build a convolutional net for image classification. This will show the standard capability of CNNs, namely learning image features. The model will consist of two components, one convolutional net to learn features in the data, and some model that uses the features to classify the input. The classification model can be anything, but it is normal to use a multilayer perceptron for this. Use Google if you want more information. Let's code it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int = 3, num_classes: int = 101) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the convolutional net\n",
    "        # I recommend 4 layers, using the parameters in the example above\n",
    "        # You can use ReLU as your activation\n",
    "        # Also, check out batch norm: https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n",
    "        self.fcn = nn.Sequential(\n",
    "            ...\n",
    "        )\n",
    "\n",
    "        # We must flatten the output from the convnet for use in the MLP\n",
    "        # There is a convenient pytorch flatten layer, but you can do it how you want\n",
    "        self.flatten = ...\n",
    "\n",
    "        # The classification head, which can be anything. You could just use a linear model for example\n",
    "        self.mlp = nn.Sequential(\n",
    "            ...\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Code the forward pass\n",
    "\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(\n",
    "        train_data,\n",
    "        validation_data,\n",
    "        model,\n",
    "        num_epochs,\n",
    "        lr,\n",
    "        batch_size\n",
    "    ):\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    validation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n",
    "    # Cross Entropy is a very standard loss function for classification\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # AdamW is an enhanced version of gradient descent \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        print(f\"STARTING EPOCH {epoch+1}\")\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "\n",
    "            images, labels = batch\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # We need to compute the loss and optimize the parameters of the model\n",
    "            # To compute the loss, we need to one-hot encode the labels, \n",
    "            # similar to what we did in the sixth cell\n",
    "\n",
    "            loss = ...\n",
    "\n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        total_labels, total_preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in validation_loader:\n",
    "\n",
    "                images, labels = batch\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "\n",
    "                # You fill in the rest\n",
    "                # You should also aggregate the predictions vs labels so we can compute the validation metrics\n",
    "                # (its similar to the train loop, only we don't want to optimize the params)\n",
    "\n",
    "                loss = ...\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Go from one-hot to label index\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                total_labels += labels\n",
    "                total_preds += preds\n",
    "\n",
    "        metrics = {\n",
    "            # Fill in your metrics (using total_labels and total_preds)\n",
    "            # You should use the accuracy score, precision, recall, and f1 scores (Google is your friend)\n",
    "            # The sklearn.metrics library includes implementations of most metrics\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> The Residual Block </h2>\n",
    "\n",
    "Before 2015, researchers discovered a strange phenomenom in deep convolutional networks. The performance on simple inputs started to degrade when making the convolutional nets deeper. Many were perplexed by this, as it was thought that the model should learn to just use what it knows about the simple patterns, and ignore what it has learnt about complex patterns, when the input is simple. \n",
    "\n",
    "What basically happens is that the model has caught all the patterns of the input before reaching the final layer, and when continuing through the next layers, the learned features become distorted. Some Microsoft researchers discovered a simple and highly efficient solution: skip the layers that aren't useful (https://arxiv.org/abs/1512.03385). This is done through residual blocks.\n",
    "\n",
    "The mechanism behind residual blocks is very simple. Simply add the input untransformed by the block to the input transformed by the block, and return the sum! This let's you skip the block. Take a look at this for a short and sweet explanation/illustration: https://paperswithcode.com/method/residual-block#:~:text=Residual%20Blocks%20are%20skip%2Dconnection,part%20of%20the%20ResNet%20architecture. \n",
    "\n",
    "This block is used everywhere, so let us code it to keep in handy:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Code your block\n",
    "        # I recommend conv + ReLU + batchnorm + dropout + conv + ReLU\n",
    "        self.block = nn.Sequential(\n",
    "            ...\n",
    "        )\n",
    "\n",
    "        # If in_channels != out_channels, you need to change the shape of the input\n",
    "        # before summing it with the block output \n",
    "        self.adapt_shape = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Go\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> New let's redo the classifier as a Residual Network </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int = 3, num_classes: int = 101) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        # Use your Residual blocks.\n",
    "        # I recommend keeping in_channels=out_channels for all the Residual Blocks,\n",
    "        # and adding convolutional downsampling layers between the blocks.\n",
    "        # With this scheme, the convolutions in the ResNet blocks should not alter the shape of the input.\n",
    "        # But you can do what you want\n",
    "        self.fcn = nn.Sequential(\n",
    "            ...\n",
    "        )\n",
    "\n",
    "        self.flatten = ...\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            ...\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Code the forward pass\n",
    "\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResnetClassifier(3, 101)\n",
    "# train_classifier(train_data, validation_data, resnet, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Autoencoders </h1>\n",
    "\n",
    "Now we are past the basic intro to CNNs that you can find everywhere, it is time to dig a little deeper.\n",
    "\n",
    "Autoencoders are a class of models where you encode the image into a much lower dimensional vector (latent representation), that you then decode to reconstruct the original image. The idea is that a model able to compress an image into a very small vector, that can then be used to reconstruct the image, will lead to an informative feature representation in that vector (https://paperswithcode.com/method/autoencoder). It is also a nice compression algorithm. Now let's build it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, latent_dim: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Go ham.\n",
    "        # I recommend using the resnet blocks \n",
    "        # (this can be identical to what you have in the ResnetClassifier)\n",
    "        self.encoder = nn.Sequential(\n",
    "            ...\n",
    "        )\n",
    "\n",
    "        # This network should basically be the inverse of the encoder.\n",
    "        # To build this, you should look into transposed convolutions:\n",
    "        # https://www.geeksforgeeks.org/what-is-transposed-convolutional-layer/\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html\n",
    "        # I recommend keeping in_channels=out_channels for the Res blocks as before, \n",
    "        # and then interleave the blocks with transposed convolutions\n",
    "        self.decoder = nn.Sequential(\n",
    "            ...\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Implement it \n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae(\n",
    "        train_data,\n",
    "        validation_data,\n",
    "        model,\n",
    "        num_epochs,\n",
    "        lr,\n",
    "        batch_size\n",
    "    ):\n",
    "\n",
    "    # Fill it in yourself\n",
    "    # You can copy most of the classification training code in\n",
    "    \n",
    "    # We use the mean squared error loss since we are training a reconstruction model\n",
    "    loss_func = nn.MSELoss()\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Finally, there is something else called a Variational Autoencoder that is a much better than a standard AE, and it can easily be extended from this. You can look into it if you want. </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> The U-Net </h1>\n",
    "\n",
    "Now let's move away from the task of image reconstruction, and turn to image segmentation. Image segmentation is pixelwise classification. For example, output 1 where the pixels show a dog in the image, and 0 else. The output shape is naturally the same as the input shape for this task.\n",
    "\n",
    "You can use many different architectures for image segmentation, but (arguably) the best one is the U-Net. It takes its name from its shape, since it has a U shape. The autoencoder also has a U shape, and the networks are remarkably similar. There is only one difference: skip-connections.\n",
    "\n",
    "Skip-connections are connections between a block in the encoder network, and the corresponding block in the decoder. The purpose of this connection is to not lose any information when downsampling and encoding the image (contrary to AEs, we don't want an information bottleneck. The point is not compression or feature learning; it is to detect interesting pixels in an image). There are many ways to implement this.\n",
    "\n",
    "Take a look at this for a concise description of a U-Net: https://paperswithcode.com/method/u-net. Due to the similarities with an AE, you should be able to recycle most of the code. The only difference will be the skip-connections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, out_classes: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Create your encoder.\n",
    "        # Maybe it is best to create a separate class to handle\n",
    "        # the skip-connections, instead of nn.Sequential?\n",
    "        self.encoder = ...\n",
    "\n",
    "        self.decoder = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Implement it \n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> We need a segmentation dataset to train this model. Let's use Oxford-IIIT Pets</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets_dataset = OxfordIIITPet(\n",
    "    \"data\", \n",
    "    target_types=\"segmentation\", \n",
    "    # transform= transforms.Compose([transforms.ToTensor(), transforms.Resize((256, 256))]),\n",
    "    download=True)\n",
    "\n",
    "example_image, example_mask = pets_dataset[0]\n",
    "example_mask = np.asarray(example_mask)\n",
    "\n",
    "_, ax = plt.subplots(ncols=2)\n",
    "ax[0].imshow(example_image)\n",
    "ax[1].imshow(example_mask);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unet(\n",
    "        train_data,\n",
    "        validation_data,\n",
    "        model,\n",
    "        num_epochs,\n",
    "        lr,\n",
    "        batch_size\n",
    "):\n",
    "    # Fill it in yourself\n",
    "    # It will be very similar to the classification training code\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Diffusion </h1>\n",
    "\n",
    "Let's do this another time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
